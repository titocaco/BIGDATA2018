{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "\n",
    "# **Lab 5b - k-Means para Quantização de Atributos**\n",
    "\n",
    "#### Os algoritmos de agrupamento de dados, além de serem utilizados em análise exploratória para extrair padrões de similaridade entre os objetos, pode ser utilizado para compactar o espaço de dados.\n",
    "\n",
    "#### Neste notebook vamos utilizar nossa base de dados de Sentiment Movie Reviews para os experimentos. Primeiro iremos utilizar a técnica word2vec que aprende uma transformação dos tokens de uma base em um vetor de atributos. Em seguida, utilizaremos o algoritmo k-Means para compactar a informação desses atributos e projetar cada objeto em um espaço de atributos de tamanho fixo.\n",
    "\n",
    "#### As células-exercícios iniciam com o comentário `# EXERCICIO` e os códigos a serem completados estão marcados pelos comentários `<COMPLETAR>`.\n",
    "\n",
    "#### ** Nesse notebook: **\n",
    "#### *Parte 1:* Word2Vec\n",
    "#### *Parte 2:* k-Means para quantizar os atributos\n",
    "#### *Parte 3:* Aplicando um k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 0: Preliminares**\n",
    "\n",
    "#### Para este notebook utilizaremos a base de dados Movie Reviews que será utilizada para o segundo projeto.\n",
    "\n",
    "#### A base de dados tem os campos separados por '\\t' e o seguinte formato:\n",
    "   `\"id da frase\",\"id da sentença\",\"Frase\",\"Sentimento\"`\n",
    "\n",
    "#### Para esse laboratório utilizaremos apenas o campo \"Frase\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8528 lines\n",
      "Sample line: (81776, 'It is a challenging film , if not always a narratively cohesive one .')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parseRDD(point):\n",
    "    \"\"\" Parser for the current dataset. It receives a data point and return\n",
    "        a sentence (third field).\n",
    "    Args:\n",
    "        point (str): input data point\n",
    "    Returns:\n",
    "        str: a string\n",
    "    \"\"\"    \n",
    "    data = point.split('\\t')\n",
    "    return (int(data[0]),data[2])\n",
    "\n",
    "def notempty(point):\n",
    "    \"\"\" Returns whether the point string is not empty\n",
    "    Args:\n",
    "        point (str): input string\n",
    "    Returns:\n",
    "        bool: True if it is not empty\n",
    "    \"\"\"   \n",
    "    return len(point[1])>0\n",
    "\n",
    "filename = os.path.join(\"Data\",\"MovieReviews2.tsv\")\n",
    "rawRDD = sc.textFile(filename,100)\n",
    "header = rawRDD.take(1)[0]\n",
    "\n",
    "dataRDD = (rawRDD\n",
    "           #.sample(False, 0.1, seed=42)\n",
    "           .filter(lambda x: x!=header)\n",
    "           .map(parseRDD)\n",
    "           .filter(notempty)\n",
    "           #.sample( False, 0.1, 42 )\n",
    "           )\n",
    "\n",
    "print ('Read {} lines'.format(dataRDD.count()))\n",
    "print ('Sample line: {}'.format(dataRDD.takeSample(False, 1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 1: Word2Vec**\n",
    "\n",
    "#### A técnica [word2vec][word2vec] aprende através de uma rede neural semântica uma representação vetorial de cada token em um corpus de tal forma que palavras semanticamente similares sejam similares na representação vetorial.\n",
    "\n",
    "####  O PySpark contém uma implementação dessa técnica, para aplicá-la basta passar um RDD em que cada objeto representa um documento e cada documento é representado por uma lista de tokens na ordem em que aparecem originalmente no corpus. Após o processo de treinamento, podemos transformar um token utilizando o método [`transform`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.feature) para transformar cada token em uma representaçã vetorial.\n",
    "\n",
    "#### Nesse ponto, cada objeto de nossa base será representada por uma matriz de tamanho variável.\n",
    "\n",
    "[word2vec]: https://code.google.com/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1a) Gerando RDD de tokens**\n",
    "\n",
    "#### Utilize a função de tokenização `tokenize` do Lab4d para gerar uma RDD `wordsRDD` contendo listas de tokens da nossa base original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quiet', 'introspective', 'entertaining', 'independent', 'worth', 'seeking']\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "import re\n",
    "\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "stopfile = os.path.join(\"Data\",\"stopwords.txt\")\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    string_list = re.split(split_regex, string)\n",
    "    string_list = filter(lambda w: len(w)>0, map(lambda w: w.lower(), string_list))\n",
    "    return [w for w in string_list if w not in stopwords]\n",
    "\n",
    "wordsRDD = dataRDD.map(lambda x: tokenize(x[1]))\n",
    "\n",
    "print (wordsRDD.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Tokenize a String (1a)\n",
    "assert wordsRDD.take(1)[0]==[u'quiet', u'introspective', u'entertaining', u'independent', u'worth', u'seeking'], 'lista incorreta!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1b) Aplicando transformação word2vec**\n",
    "\n",
    "#### Crie um modelo word2vec aplicando o método `fit` na RDD criada no exercício anterior.\n",
    "\n",
    "#### Para aplicar esse método deve ser fazer um pipeline de métodos, primeiro executando `Word2Vec()`, em seguida aplicando o método `setVectorSize()` com o tamanho que queremos para nosso vetor (utilize tamanho 5), seguido de `setSeed()` para a semente aleatória, em caso de experimentos controlados (utilizaremos 42) e, finalmente, `fit()` com nossa `wordsRDD` como parâmetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "model = Word2Vec() \\\n",
    "        .setVectorSize(5) \\\n",
    "        .setSeed(42) \\\n",
    "        .fit(wordsRDD)\n",
    "\n",
    "print (model.transform(u'entertaining'))\n",
    "print (list(model.findSynonyms(u'entertaining', 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.abs(model.transform(u'entertaining')-np.array([0.0136831374839,0.00371457682922,-0.135785803199,0.047585401684,0.0414853096008])).mean()\n",
    "assert dist<1e-6, 'valores incorretos'\n",
    "assert list(model.findSynonyms(u'entertaining', 1))[0][0] == 'god', 'valores incorretos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1c) Gerando uma RDD de matrizes**\n",
    "\n",
    "#### Como primeiro passo, precisamos gerar um dicionário em que a chave são as palavras e o valor é o vetor representativo dessa palavra.\n",
    "\n",
    "#### Para isso vamos primeiro gerar uma lista `uniqueWords` contendo as palavras únicas do RDD words, removendo aquelas que aparecem menos do que 5 vezes [$^1$](#1). Em seguida, criaremos um dicionário `w2v` que a chave é um token e o valor é um `np.array` do vetor transformado daquele token[$^2$](#2).\n",
    "\n",
    "#### Finalmente, vamos criar uma RDD chamada `vectorsRDD` em que cada registro é representado por uma matriz onde cada linha representa uma palavra transformada.\n",
    "\n",
    "##### 1\n",
    "Na versão 1.3 do PySpark o modelo Word2Vec utiliza apenas os tokens que aparecem mais do que 5 vezes no corpus, na versão 1.4 isso é parametrizado.\n",
    "\n",
    "##### 2\n",
    "Na versão 1.4 do PySpark isso pode ser feito utilizando o método `getVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "uniqueWords = (wordsRDD\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "               .<COMPLETAR>\n",
    "               .collect()\n",
    "               )\n",
    "\n",
    "print ('{} tokens únicos'.format(len(uniqueWords)))\n",
    "\n",
    "w2v = {}\n",
    "for w in uniqueWords:\n",
    "    w2v[w] = <COMPLETAR>\n",
    "w2vb = sc.broadcast(w2v)  # acesse como w2vb.value[w]     \n",
    "print ('Vetor entertaining: {}'.format( w2v[u'entertaining']))\n",
    "\n",
    "vectorsRDD = (wordsRDD\n",
    "              .<COMPLETAR>\n",
    "             )\n",
    "recs = vectorsRDD.take(2)\n",
    "firstRec, secondRec = recs[0], recs[1]\n",
    "print (firstRec.shape, secondRec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Tokenizing the small datasets (1c)\n",
    "assert len(uniqueWords) == 3388,  'valor incorreto'\n",
    "assert np.mean(np.abs(w2v[u'entertaining']-[0.0136831374839,0.00371457682922,-0.135785803199,0.047585401684,0.0414853096008]))<1e-6,'valor incorreto'\n",
    "assert secondRec.shape == (10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 2: k-Means para quantizar os atributos**\n",
    "\n",
    "#### Nesse momento é fácil perceber que não podemos aplicar nossas técnicas de aprendizado supervisionado nessa base de dados:\n",
    "\n",
    "  * #### A regressão logística requer um vetor de tamanho fixo representando cada objeto\n",
    "  * #### O k-NN necessita uma forma clara de comparação entre dois objetos, que métrica de similaridade devemos aplicar?\n",
    "  \n",
    "#### Para resolver essa situação, vamos executar uma nova transformação em nossa RDD.   Primeiro vamos aproveitar o fato de que dois tokens com significado similar são mapeados em vetores similares, para agrupá-los em um atributo único.\n",
    "\n",
    "#### Ao aplicarmos o k-Means nesse conjunto de vetores, podemos criar $k$ pontos representativos e, para cada documento, gerar um histograma de contagem de tokens nos clusters gerados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2a) Agrupando os vetores e criando centros representativos**\n",
    "\n",
    "#### Como primeiro passo vamos gerar um RDD com os valores do dicionário `w2v`. Em seguida, aplicaremos o algoritmo `k-Means` com $k = 200$ e $seed = 42$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "from  pyspark.mllib.clustering import KMeans\n",
    "\n",
    "vectors2RDD = sc.parallelize(np.array(list(w2v.values())),1)\n",
    "print ('Sample vector: {}'.format(vectors2RDD.take(1)))\n",
    "\n",
    "modelK = KMeans.<COMPLETAR>\n",
    "\n",
    "clustersRDD = vectors2RDD.<COMPLETAR>\n",
    "print ('10 first clusters allocation: {}'.format(clustersRDD.take(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Amazon record with the most tokens (1d)\n",
    "assert clustersRDD.take(10)==[142, 83, 42, 0, 87, 52, 190, 17, 56, 0], 'valor incorreto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2b) Transformando matriz de dados em vetores quantizados**\n",
    "\n",
    "#### O próximo passo consiste em transformar nosso RDD de frases em um RDD de pares (id, vetor quantizado). Para isso vamos criar uma função quantizador que receberá como parâmetros o objeto, o modelo de k-means, o valor de k e o dicionário word2vec.\n",
    "\n",
    "#### Para cada ponto, vamos separar o id e aplicar a função `tokenize` na string. Em seguida, transformamos a lista de tokens em uma matriz word2vec. Finalmente, aplicamos cada vetor dessa matriz no modelo de k-Means, gerando um vetor de tamanho $k$ em que cada posição $i$ indica quantos tokens pertencem ao cluster $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "def quantizador(point, model, k, w2v):\n",
    "    key = <COMPLETAR>\n",
    "    words = <COMPLETAR>\n",
    "    matrix = np.array( <COMPLETAR> )\n",
    "    features = np.zeros(k)\n",
    "    for v in matrix:\n",
    "        c = <COMPLETAR>\n",
    "        features[c] += 1\n",
    "    return (key, features)\n",
    "    \n",
    "quantRDD = dataRDD.map(lambda x: quantizador(x, modelK, 500, w2v))\n",
    "\n",
    "print quantRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Implement a TF function (2a)\n",
    "assert quantRDD.take(1)[0][1].sum() == 5, 'valores incorretos'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
